Comment on [Datanauts 7-Decoupled Storage Architecture](http://packetpushers.net/podcast/podcasts/datanauts-007-decoupled-storage-architecture/)

Interesting show with a funny guest. It’s a worthwhile topic, too, since, conceptually, distributed cache brings up a lot of questions. I followed up a bit on the Pernix web site without a lot of clarification in the time I had to review their documentation, so I’m mostly going on what was said in the podcast. Of course, as always, you’re time constrained in everything the podcast can cover.

Anytime I hear “distributed,” especially in terms of database, I want to be clear about event ordering. For my questions, assume some virtualized transaction apps that are reading and writing to a backend database, possibly virtualized. 

Say that Application A on VM1 commits a write. As I understand it, the write hits the Pernix cache and a local copy is stored with a scope of all VMs using the data. Then the write is replicated to the database where it’s committed, possibly by two phase commit. Pernix apparently doesn’t care how the data gets committed. 

Subsequently,  App A on VM1 reads the data. In this simple case, we expect that App A on VM1 will get the data from the Pernix flash since Pernix knows the cached data is fresh.

Now, as a second case, assume App A on VM1 does a write, as earlier. A scale out copy of App A on a different VM, VM2, also does a write. How does Pernix arbitrate which write wins? We have to assume the clocks aren’t exactly the same. App B on yet another scale out VM, VM3, does a read. How does Pernix guarantee that App B gets the correct data?

All this assumes no failures across the transaction write chains, including, in distributed computing terms, no “network partitions.” What happens when there are transient or other network partitions, especially between scale out VMs running apps and their associated databases? 

Similarly, the Pernix cache is fast, which is a lot of its value. The path from the cache to the storage device has relatively long latency, even when everything is working correctly. What happens if there’s an interruption on the path from the cache to the storage and the commit doesn’t happen? What’s the correct data? Where is it?

Many business apps, especially legacy apps, run stored procedures in databases to perform business logic operations. How do stored procedures affect Pernix, if at all?

Some other notes: It was a great question around 21:20 about what does it take to introduce this tech into a datacenter, the Day 1 considerations. I understand that one adds the software and flash to the scope of one or more VMs, but wasn’t clear, for instance, if there’s a pass through mode where one can turn off the Pernix software in case it’s felt to be a problem. Is there’s a modelling mode where one can see which workloads get the most value? 

Since the Pernix software apparently operates at a low level in the hypervisor, how sensitive is it to hypervisor versions, releases, patches?

Around 41:59 I didn’t understand the point about hybrid storage not being deterministic. I don’t necessarily think that it is, or that any storage is deterministic, but the point might have been worth exploring.

Around 46:58 “datagravity” was mentioned. I wasn’t sure what datagravity meant in this specific context. (I know what datagravity is.)

Last, one thing that might have been worth further discussion (in the time you didn’t have) was whether the Pernix software could be decoupled from the hardware, so to speak, and be used in something like an AWS context. 

Great show investigating a worthwhile technology. I’m especially interested in anything that’s distributed or enables scale out.

####References
[Databases with PernixData FVP Software Overview](http://pernixdata.com/resource/databases-pernixdata-fvp-software-overview)

    
